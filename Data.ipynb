{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj2Ts86JEOrm"
      },
      "outputs": [],
      "source": [
        "# Retrieval-Augmented Generation (RAG) System\n",
        "\n",
        "# Import Libraries\n",
        "# ----------------\n",
        "\n",
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "!pip install pypdf\n",
        "!pip install chromadb\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "import os\n",
        "from google.colab import drive\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Constants and API Keys\n",
        "# ----------------------\n",
        "OPENAI_API_KEY = \"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\"  # Replace with your actual API key\n",
        "#NVIDIA_PDF_PATH = \"/content/drive/MyDrive/LLM\"\n",
        "# Path to the directory containing the PDF files\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "directory_path = \"/content/drive/My Drive/LLM\"\n",
        "\n",
        "# List all files in the directory\n",
        "all_files = os.listdir(directory_path)\n",
        "\n",
        "# Filter out only PDF files\n",
        "pdf_files = [file for file in all_files if file.endswith('.pdf')]\n",
        "\n",
        "# Process each PDF file\n",
        "for pdf_file in pdf_files:\n",
        "    pdf_path = os.path.join(directory_path, pdf_file)\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    pages = loader.load_and_split()\n",
        "\n",
        "    # Now 'pages' contains the content of the current PDF file\n",
        "    # Add your processing code here\n",
        "VECTOR_DB_DIRECTORY = \"/content/vectordb\"\n",
        "GPT_MODEL_NAME = 'gpt-3.5-turbo'\n",
        "CHUNK_SIZE = 700\n",
        "CHUNK_OVERLAP = 50\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmjJRnUKG4eI",
        "outputId": "3678be6b-38be-441e-89c3-f51fd394233c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function Definitions\n",
        "# --------------------\n",
        "\n",
        "def mount_google_drive():\n",
        "    \"\"\"Mounts Google Drive for accessing files.\"\"\"\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "#def load_and_split_document(pdf_path):\n",
        "    \"\"\"Loads and splits the document into pages.\"\"\"\n",
        "  #  loader = PyPDFLoader(pdf_path)\n",
        "  #  return loader.load_and_split()\n",
        "\n",
        "def split_text_into_chunks(pages, chunk_size, chunk_overlap):\n",
        "    \"\"\"Splits text into smaller chunks for processing.\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    return text_splitter.split_documents(pages)\n",
        "\n",
        "def create_embeddings(api_key):\n",
        "    \"\"\"Creates embeddings from text.\"\"\"\n",
        "    return OpenAIEmbeddings(openai_api_key=api_key)\n",
        "\n",
        "def setup_vector_database(documents, embeddings, directory):\n",
        "    \"\"\"Sets up a vector database for storing embeddings.\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    return Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=directory)\n",
        "\n",
        "def initialize_chat_model(api_key, model_name):\n",
        "    \"\"\"Initializes the chat model with specified AI model.\"\"\"\n",
        "    return ChatOpenAI(openai_api_key=api_key, model_name=model_name, temperature=0.0)\n",
        "\n",
        "def create_retrieval_qa_chain(chat_model, vector_database):\n",
        "    \"\"\"Creates a retrieval QA chain combining model and database.\"\"\"\n",
        "    memory = ConversationBufferWindowMemory(memory_key='chat_history', k=5, return_messages=True)\n",
        "    return ConversationalRetrievalChain.from_llm(chat_model, retriever=vector_database.as_retriever(), memory=memory)\n",
        "\n",
        "def ask_question_and_get_answer(qa_chain, question):\n",
        "    \"\"\"Asks a question and retrieves the answer.\"\"\"\n",
        "    return qa_chain({\"question\": question})['answer']\n",
        "\n"
      ],
      "metadata": {
        "id": "kVRyPLHSHUQc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Execution Flow\n",
        "# -------------------\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to execute the RAG workflow.\"\"\"\n",
        "    mount_google_drive()\n",
        "   # pages = load_and_split_document(NVIDIA_PDF_PATH)\n",
        "\n",
        "    documents = split_text_into_chunks(pages, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "    embeddings = create_embeddings(OPENAI_API_KEY)\n",
        "    vector_database = setup_vector_database(documents, embeddings, VECTOR_DB_DIRECTORY)\n",
        "    chat_model = initialize_chat_model(OPENAI_API_KEY, GPT_MODEL_NAME)\n",
        "    qa_chain = create_retrieval_qa_chain(chat_model, vector_database)\n",
        "\n",
        "    # Sample Questions\n",
        "    questions = [\n",
        "        \"What is th reason of black smoke in funnel?\",\n",
        "        # ... Additional questions can be added here\n",
        "    ]\n",
        "\n",
        "    # Process Questions and Answers\n",
        "    for question in questions:\n",
        "        answer = ask_question_and_get_answer(qa_chain, question)\n",
        "        print(f\"Question: {question}\\nAnswer: {answer}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ddFMtQCOKk8_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
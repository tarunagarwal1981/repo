# -*- coding: utf-8 -*-
"""Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jkPtQqoZKUI69ORnI609B4HTEuMmlOif
"""

# Retrieval-Augmented Generation (RAG) System

# Import Libraries
# ----------------

!pip install langchain
!pip install langchain_community
!pip install pypdf
!pip install chromadb
!pip install openai
!pip install tiktoken
import os
from google.colab import drive
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationalRetrievalChain

# Constants and API Keys
# ----------------------
OPENAI_API_KEY = "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"  # Replace with your actual API key
#NVIDIA_PDF_PATH = "/content/drive/MyDrive/LLM"
# Path to the directory containing the PDF files

from google.colab import drive
drive.mount('/content/drive')
directory_path = "/content/drive/My Drive/LLM"

# List all files in the directory
all_files = os.listdir(directory_path)

# Filter out only PDF files
pdf_files = [file for file in all_files if file.endswith('.pdf')]

# Process each PDF file
for pdf_file in pdf_files:
    pdf_path = os.path.join(directory_path, pdf_file)
    loader = PyPDFLoader(pdf_path)
    pages = loader.load_and_split()

    # Now 'pages' contains the content of the current PDF file
    # Add your processing code here
VECTOR_DB_DIRECTORY = "/content/vectordb"
GPT_MODEL_NAME = 'gpt-3.5-turbo'
CHUNK_SIZE = 700
CHUNK_OVERLAP = 50

# Function Definitions
# --------------------

def mount_google_drive():
    """Mounts Google Drive for accessing files."""
    drive.mount('/content/drive')

#def load_and_split_document(pdf_path):
    """Loads and splits the document into pages."""
  #  loader = PyPDFLoader(pdf_path)
  #  return loader.load_and_split()

def split_text_into_chunks(pages, chunk_size, chunk_overlap):
    """Splits text into smaller chunks for processing."""
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return text_splitter.split_documents(pages)

def create_embeddings(api_key):
    """Creates embeddings from text."""
    return OpenAIEmbeddings(openai_api_key=api_key)

def setup_vector_database(documents, embeddings, directory):
    """Sets up a vector database for storing embeddings."""
    if not os.path.exists(directory):
        os.makedirs(directory)
    return Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=directory)

def initialize_chat_model(api_key, model_name):
    """Initializes the chat model with specified AI model."""
    return ChatOpenAI(openai_api_key=api_key, model_name=model_name, temperature=0.0)

def create_retrieval_qa_chain(chat_model, vector_database):
    """Creates a retrieval QA chain combining model and database."""
    memory = ConversationBufferWindowMemory(memory_key='chat_history', k=5, return_messages=True)
    return ConversationalRetrievalChain.from_llm(chat_model, retriever=vector_database.as_retriever(), memory=memory)

def ask_question_and_get_answer(qa_chain, question):
    """Asks a question and retrieves the answer."""
    return qa_chain({"question": question})['answer']

# Main Execution Flow
# -------------------

def main():
    """Main function to execute the RAG workflow."""
    mount_google_drive()
   # pages = load_and_split_document(NVIDIA_PDF_PATH)

    documents = split_text_into_chunks(pages, CHUNK_SIZE, CHUNK_OVERLAP)
    embeddings = create_embeddings(OPENAI_API_KEY)
    vector_database = setup_vector_database(documents, embeddings, VECTOR_DB_DIRECTORY)
    chat_model = initialize_chat_model(OPENAI_API_KEY, GPT_MODEL_NAME)
    qa_chain = create_retrieval_qa_chain(chat_model, vector_database)

    # Sample Questions
    questions = [
        "What is th reason of black smoke in funnel?",
        # ... Additional questions can be added here
    ]

    # Process Questions and Answers
    for question in questions:
        answer = ask_question_and_get_answer(qa_chain, question)
        print(f"Question: {question}\nAnswer: {answer}\n")

if __name__ == "__main__":
    main()